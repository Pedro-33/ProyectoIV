    "from sklearn.linear_model import LogisticRegression\n",
    "modelo_1 = LogisticRegression(solver = 'saga', random_state = 123, class_weight='balanced', max_iter=1000)\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'l1_ratio': [0, 0.5, 1]\n",
    "clf_1 = GridSearchCV(estimator = modelo_1, param_grid = param_grid, cv = 5, scoring = 'f1_weighted') #Optimizamos directamente el F1 ponderado.\n",
    "clf_1.fit(X_train, y_train)\n"
    "                        pd.DataFrame(clf_1.cv_results_['mean_test_score'], columns = ['f1'])], axis = 1)\n",
    "validation.sort_values(by = 'f1', ascending = False)\n"
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier optimizado para F1\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_grid = {'n_estimators': [100, 200],\n",
    "              'max_depth': [None, 5, 10],\n",
    "              'max_features': ['sqrt', 'log2']}\n",
    "rf_model = GridSearchCV(RandomForestClassifier(random_state=123), param_grid, cv=5, scoring='f1_weighted')\n",
    "rf_model.fit(X_train, y_train.values.ravel())\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print('Metricas de desempe√±o de testeo (Random Forest)')\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "sb.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d')\n",
    "plt.show()\n"
   ]
  },
    "print(classification_report(y_test, y_pred_test_2))\n",
    "sb.heatmap(confusion_matrix(y_test, y_pred_test_2), annot = True, fmt = 'd')\n",
    "plt.show()\n"
}